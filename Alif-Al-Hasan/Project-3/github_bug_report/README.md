# Toxicity Patterns in GitHub Bug Report Discussions

## Overview

This part analyzes toxicity in GitHub issue comments from bug report discussions. We compare two automated toxicity detection methods (Detoxify and LLM-based) and discover distinct patterns of toxicity through clustering analysis.

---

## Project Structure

```
github_bug_report/
│
├── data/                             # Data directory
│   ├── bug_reports.csv               # Download from Google Drive (see below)
│   ├── ground_truths.csv             # Generated by manual labeling
│   └── results.csv                   # Generated by detoxify and llama
│
├── toxicity_detectors/               # Execution scripts
│   ├── run_detoxify.py               # Python script to run detoxify
│   └── run_llama.py                  # Python script to run llama
│
├── data_preprocessor.py              # Preprocesses the data
├── README.md                         # This file
```

---

## Dataset

### Download Instructions

**The dataset is too large for GitHub. Please download it from Google Drive:**

**[Download bug_reports.csv from Google Drive](https://drive.google.com/file/d/1zSzpa-qXKSJR1uZ62VaSCaIqEM-Lezxc/view?usp=sharing)**

After downloading:
1. Place `bug_reports.csv` inside the `data/` folder